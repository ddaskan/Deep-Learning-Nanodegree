{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [01:38, 1.73MB/s]                                                                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 4:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPQAUpugBoDBFDwCFKXoAKEzRA0Bh/xsLZO6u02DPIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9b3f5d1668>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 4\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255 # because rgb is between 0-255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    labels = range(10)\n",
    "    out = np.zeros([len(x), len(labels)])\n",
    "    for i, label in enumerate(x):\n",
    "        out[i, label] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = [None] + list(image_shape)\n",
    "    return tf.placeholder(tf.float32, shape=tensor_shape, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = [None, n_classes]\n",
    "    return tf.placeholder(tf.float32, shape=tensor_shape, name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    dimension = x_tensor.shape.as_list()\n",
    "    \n",
    "    # these two both passes the test, why is conv_ksize needed? and why does the other one pass?\n",
    "    w_shape = list(conv_ksize) + [dimension[-1]] + [conv_num_outputs] # [2, 2, 5, 10]\n",
    "    #w_shape = dimension[1:] + [conv_num_outputs] # [32, 32, 5, 10]\n",
    "    #print(w_shape)\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal(w_shape, stddev=0.1))\n",
    "    b = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    x = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    \n",
    "    x = tf.nn.relu(x)\n",
    "    \n",
    "    x = tf.nn.max_pool(x, \n",
    "                       ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                       strides=[1, pool_strides[0], pool_strides[1], 1], \n",
    "                       padding='SAME')\n",
    "    return x \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs=num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #return tf.contrib.layers.fully_connected(x_tensor, num_outputs=num_outputs)\n",
    "    dimension = x_tensor.get_shape().as_list()\n",
    "    shape = [dimension[-1], num_outputs]\n",
    "    weight = tf.Variable(tf.truncated_normal(shape, 0, 0.01))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    x_out = conv2d_maxpool(x, 20, (4, 4), (2, 2), (6, 6), (2, 2))\n",
    "    x_out = tf.nn.dropout(x_out, keep_prob)\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    x_out = flatten(x_out)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    x_out = fully_conn(x_out, 300)\n",
    "    x_out = tf.nn.dropout(x_out, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    x_out = output(x_out, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return x_out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "\n",
    "    print('Loss:', loss, ' Validation Accuracy:', valid_acc)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.1475  Validation Accuracy: 0.247\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.92554  Validation Accuracy: 0.344\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.81204  Validation Accuracy: 0.3858\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.72401  Validation Accuracy: 0.4036\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.64786  Validation Accuracy: 0.4266\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.58283  Validation Accuracy: 0.4456\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.53262  Validation Accuracy: 0.454\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.49504  Validation Accuracy: 0.4694\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.45942  Validation Accuracy: 0.4716\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.42055  Validation Accuracy: 0.4764\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.38481  Validation Accuracy: 0.4896\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.35891  Validation Accuracy: 0.4968\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.33416  Validation Accuracy: 0.5014\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.30481  Validation Accuracy: 0.5122\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.2882  Validation Accuracy: 0.5138\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.27178  Validation Accuracy: 0.517\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.24516  Validation Accuracy: 0.5258\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.22935  Validation Accuracy: 0.527\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.20315  Validation Accuracy: 0.5352\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.18394  Validation Accuracy: 0.534\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.17139  Validation Accuracy: 0.5368\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.1556  Validation Accuracy: 0.5434\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.13367  Validation Accuracy: 0.5448\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.11757  Validation Accuracy: 0.5488\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.10538  Validation Accuracy: 0.5538\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.10254  Validation Accuracy: 0.5506\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.07821  Validation Accuracy: 0.5582\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.07189  Validation Accuracy: 0.5582\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.05146  Validation Accuracy: 0.5616\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.03934  Validation Accuracy: 0.5594\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.01797  Validation Accuracy: 0.5686\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.00874  Validation Accuracy: 0.5692\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.995876  Validation Accuracy: 0.5742\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.985455  Validation Accuracy: 0.5802\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.970492  Validation Accuracy: 0.5772\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.958314  Validation Accuracy: 0.5796\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.949524  Validation Accuracy: 0.581\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.927727  Validation Accuracy: 0.5868\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.930657  Validation Accuracy: 0.5874\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.905024  Validation Accuracy: 0.5824\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.910702  Validation Accuracy: 0.5846\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.901369  Validation Accuracy: 0.5818\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.877121  Validation Accuracy: 0.588\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.868637  Validation Accuracy: 0.5838\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.859926  Validation Accuracy: 0.5894\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.853319  Validation Accuracy: 0.5898\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.838183  Validation Accuracy: 0.5896\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.817656  Validation Accuracy: 0.5982\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.812597  Validation Accuracy: 0.5928\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.806926  Validation Accuracy: 0.593\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.796797  Validation Accuracy: 0.5916\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.774848  Validation Accuracy: 0.5972\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.780759  Validation Accuracy: 0.594\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.758513  Validation Accuracy: 0.593\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.752303  Validation Accuracy: 0.5974\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.742386  Validation Accuracy: 0.593\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.741359  Validation Accuracy: 0.5974\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.717154  Validation Accuracy: 0.598\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.71346  Validation Accuracy: 0.5984\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.693606  Validation Accuracy: 0.599\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.692446  Validation Accuracy: 0.5984\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.679944  Validation Accuracy: 0.6016\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.675743  Validation Accuracy: 0.6052\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.660645  Validation Accuracy: 0.5994\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.658736  Validation Accuracy: 0.5932\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.647509  Validation Accuracy: 0.5996\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.641497  Validation Accuracy: 0.6018\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.639472  Validation Accuracy: 0.6026\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.625554  Validation Accuracy: 0.6056\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.6132  Validation Accuracy: 0.5972\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.591121  Validation Accuracy: 0.608\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.599841  Validation Accuracy: 0.6018\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.597019  Validation Accuracy: 0.6016\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.578042  Validation Accuracy: 0.6028\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.582941  Validation Accuracy: 0.6004\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.574978  Validation Accuracy: 0.6004\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.551547  Validation Accuracy: 0.608\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.552038  Validation Accuracy: 0.5966\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.566638  Validation Accuracy: 0.5952\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.541109  Validation Accuracy: 0.6048\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.526986  Validation Accuracy: 0.6124\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.530038  Validation Accuracy: 0.6122\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.497042  Validation Accuracy: 0.6108\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.501228  Validation Accuracy: 0.6124\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.500898  Validation Accuracy: 0.6092\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.490398  Validation Accuracy: 0.614\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.473147  Validation Accuracy: 0.6156\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.480876  Validation Accuracy: 0.6122\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.469386  Validation Accuracy: 0.6096\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.451204  Validation Accuracy: 0.6116\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.44818  Validation Accuracy: 0.6138\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.437216  Validation Accuracy: 0.6102\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.435583  Validation Accuracy: 0.6136\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.433713  Validation Accuracy: 0.614\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.425054  Validation Accuracy: 0.6136\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.418669  Validation Accuracy: 0.614\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.409348  Validation Accuracy: 0.6138\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.406515  Validation Accuracy: 0.6096\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.392258  Validation Accuracy: 0.6152\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.377169  Validation Accuracy: 0.6146\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.11918  Validation Accuracy: 0.2426\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 1.87328  Validation Accuracy: 0.334\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.73179  Validation Accuracy: 0.3658\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.63089  Validation Accuracy: 0.3896\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.67423  Validation Accuracy: 0.4022\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.68183  Validation Accuracy: 0.4256\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.55674  Validation Accuracy: 0.4476\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.42096  Validation Accuracy: 0.443\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.43495  Validation Accuracy: 0.4684\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.49055  Validation Accuracy: 0.4674\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.52968  Validation Accuracy: 0.4818\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.46487  Validation Accuracy: 0.4764\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.32482  Validation Accuracy: 0.4852\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.33944  Validation Accuracy: 0.5008\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.39338  Validation Accuracy: 0.504\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.43232  Validation Accuracy: 0.5024\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.38325  Validation Accuracy: 0.5036\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.25593  Validation Accuracy: 0.5072\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.27538  Validation Accuracy: 0.5134\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.32147  Validation Accuracy: 0.5242\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.36893  Validation Accuracy: 0.527\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.33049  Validation Accuracy: 0.5176\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.20076  Validation Accuracy: 0.531\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.21462  Validation Accuracy: 0.5402\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.2692  Validation Accuracy: 0.534\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.33484  Validation Accuracy: 0.5396\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.2892  Validation Accuracy: 0.5354\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.15827  Validation Accuracy: 0.5444\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.1637  Validation Accuracy: 0.5538\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.23222  Validation Accuracy: 0.5482\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.29543  Validation Accuracy: 0.5606\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.23921  Validation Accuracy: 0.5502\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.12068  Validation Accuracy: 0.56\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.12174  Validation Accuracy: 0.5666\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.18944  Validation Accuracy: 0.5624\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.25657  Validation Accuracy: 0.568\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.20735  Validation Accuracy: 0.5604\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.07742  Validation Accuracy: 0.56\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.09567  Validation Accuracy: 0.5756\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.16483  Validation Accuracy: 0.5672\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.24898  Validation Accuracy: 0.5678\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.1865  Validation Accuracy: 0.5688\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.07215  Validation Accuracy: 0.574\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.07337  Validation Accuracy: 0.5804\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.12821  Validation Accuracy: 0.5734\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.20507  Validation Accuracy: 0.5776\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.14806  Validation Accuracy: 0.5816\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.04191  Validation Accuracy: 0.579\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.04971  Validation Accuracy: 0.5884\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.11072  Validation Accuracy: 0.579\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.18269  Validation Accuracy: 0.581\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.13532  Validation Accuracy: 0.5814\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.02271  Validation Accuracy: 0.5828\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.02266  Validation Accuracy: 0.5922\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.0782  Validation Accuracy: 0.584\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.17317  Validation Accuracy: 0.5886\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.11771  Validation Accuracy: 0.5854\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.00105  Validation Accuracy: 0.587\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.01106  Validation Accuracy: 0.5936\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.05175  Validation Accuracy: 0.5938\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.13877  Validation Accuracy: 0.5954\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.0822  Validation Accuracy: 0.5988\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.986044  Validation Accuracy: 0.5924\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.985296  Validation Accuracy: 0.6024\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.03652  Validation Accuracy: 0.599\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.11822  Validation Accuracy: 0.598\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.0653  Validation Accuracy: 0.5976\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.955794  Validation Accuracy: 0.5986\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.964911  Validation Accuracy: 0.6056\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.01125  Validation Accuracy: 0.6024\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.10841  Validation Accuracy: 0.6\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.03982  Validation Accuracy: 0.6036\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.952431  Validation Accuracy: 0.5978\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.94803  Validation Accuracy: 0.6088\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.986499  Validation Accuracy: 0.609\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.08245  Validation Accuracy: 0.6072\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.02973  Validation Accuracy: 0.607\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.928351  Validation Accuracy: 0.607\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.930626  Validation Accuracy: 0.6142\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.977114  Validation Accuracy: 0.607\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.0638  Validation Accuracy: 0.6146\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.01484  Validation Accuracy: 0.613\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.904194  Validation Accuracy: 0.6092\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.917404  Validation Accuracy: 0.6118\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.962032  Validation Accuracy: 0.6146\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.04369  Validation Accuracy: 0.6138\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.00318  Validation Accuracy: 0.6184\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.897622  Validation Accuracy: 0.6102\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.899599  Validation Accuracy: 0.6198\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.945045  Validation Accuracy: 0.6138\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.01061  Validation Accuracy: 0.6174\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.989386  Validation Accuracy: 0.6162\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.890004  Validation Accuracy: 0.6132\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.885888  Validation Accuracy: 0.6208\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.923359  Validation Accuracy: 0.621\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.00045  Validation Accuracy: 0.6162\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.965989  Validation Accuracy: 0.6232\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.877986  Validation Accuracy: 0.6218\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.873288  Validation Accuracy: 0.622\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.916305  Validation Accuracy: 0.625\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.980154  Validation Accuracy: 0.6242\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.961391  Validation Accuracy: 0.6272\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.85533  Validation Accuracy: 0.6214\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.854105  Validation Accuracy: 0.6318\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.88876  Validation Accuracy: 0.6288\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.954223  Validation Accuracy: 0.6268\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.939589  Validation Accuracy: 0.6326\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.841277  Validation Accuracy: 0.6226\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.832701  Validation Accuracy: 0.6344\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.879035  Validation Accuracy: 0.6288\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.93443  Validation Accuracy: 0.629\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.925944  Validation Accuracy: 0.6354\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.83487  Validation Accuracy: 0.6288\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.833162  Validation Accuracy: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.865255  Validation Accuracy: 0.627\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.94127  Validation Accuracy: 0.6322\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.916407  Validation Accuracy: 0.6436\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.842996  Validation Accuracy: 0.6298\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.797344  Validation Accuracy: 0.6424\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.850619  Validation Accuracy: 0.6304\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.913395  Validation Accuracy: 0.637\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.907218  Validation Accuracy: 0.6384\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.816323  Validation Accuracy: 0.6352\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.787375  Validation Accuracy: 0.6352\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.829614  Validation Accuracy: 0.6344\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.901932  Validation Accuracy: 0.6368\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.903903  Validation Accuracy: 0.6416\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.811821  Validation Accuracy: 0.6344\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.783938  Validation Accuracy: 0.6408\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.815642  Validation Accuracy: 0.636\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.883681  Validation Accuracy: 0.6458\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.872884  Validation Accuracy: 0.6462\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.799199  Validation Accuracy: 0.6382\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.761088  Validation Accuracy: 0.6428\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.809262  Validation Accuracy: 0.6414\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.87256  Validation Accuracy: 0.6448\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.863735  Validation Accuracy: 0.6468\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.782721  Validation Accuracy: 0.6418\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.753987  Validation Accuracy: 0.6466\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.801186  Validation Accuracy: 0.6406\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.842899  Validation Accuracy: 0.645\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.854397  Validation Accuracy: 0.6446\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.777417  Validation Accuracy: 0.6422\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.744969  Validation Accuracy: 0.646\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.777253  Validation Accuracy: 0.636\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.836948  Validation Accuracy: 0.645\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.856293  Validation Accuracy: 0.652\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.758047  Validation Accuracy: 0.6456\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.742284  Validation Accuracy: 0.647\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.770612  Validation Accuracy: 0.6426\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.819375  Validation Accuracy: 0.6532\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.843603  Validation Accuracy: 0.654\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.748541  Validation Accuracy: 0.6498\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.715517  Validation Accuracy: 0.6498\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.761683  Validation Accuracy: 0.647\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.813497  Validation Accuracy: 0.6552\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.827161  Validation Accuracy: 0.6486\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.749064  Validation Accuracy: 0.6458\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.715268  Validation Accuracy: 0.6462\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.755568  Validation Accuracy: 0.6446\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.787649  Validation Accuracy: 0.657\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.814147  Validation Accuracy: 0.6568\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.732228  Validation Accuracy: 0.6454\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.706443  Validation Accuracy: 0.6496\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.730972  Validation Accuracy: 0.6488\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.781983  Validation Accuracy: 0.6572\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.799888  Validation Accuracy: 0.6542\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.733226  Validation Accuracy: 0.6544\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.692036  Validation Accuracy: 0.6562\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.729696  Validation Accuracy: 0.6498\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.765985  Validation Accuracy: 0.6586\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.802365  Validation Accuracy: 0.658\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.716409  Validation Accuracy: 0.651\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.668109  Validation Accuracy: 0.655\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.713062  Validation Accuracy: 0.6536\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.760208  Validation Accuracy: 0.6568\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.783227  Validation Accuracy: 0.6554\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.702722  Validation Accuracy: 0.6544\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.649971  Validation Accuracy: 0.6604\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.703269  Validation Accuracy: 0.6618\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.751577  Validation Accuracy: 0.6652\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.784381  Validation Accuracy: 0.656\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.690808  Validation Accuracy: 0.6572\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.655959  Validation Accuracy: 0.6634\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.684087  Validation Accuracy: 0.657\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.743928  Validation Accuracy: 0.6612\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.764761  Validation Accuracy: 0.6582\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.689537  Validation Accuracy: 0.6578\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.639762  Validation Accuracy: 0.6568\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.683982  Validation Accuracy: 0.6568\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.735043  Validation Accuracy: 0.6602\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.767532  Validation Accuracy: 0.6628\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.661107  Validation Accuracy: 0.661\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.631524  Validation Accuracy: 0.664\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.671649  Validation Accuracy: 0.662\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.716325  Validation Accuracy: 0.6656\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.74786  Validation Accuracy: 0.6624\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.668648  Validation Accuracy: 0.661\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.626335  Validation Accuracy: 0.6606\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.677716  Validation Accuracy: 0.6588\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.708183  Validation Accuracy: 0.6694\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.734166  Validation Accuracy: 0.661\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.638692  Validation Accuracy: 0.6624\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.62217  Validation Accuracy: 0.6686\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.663268  Validation Accuracy: 0.6586\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.70689  Validation Accuracy: 0.664\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.716273  Validation Accuracy: 0.6654\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.655663  Validation Accuracy: 0.6622\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.604644  Validation Accuracy: 0.659\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.650528  Validation Accuracy: 0.6598\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.686888  Validation Accuracy: 0.6652\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.717584  Validation Accuracy: 0.6634\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.632111  Validation Accuracy: 0.658\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.595535  Validation Accuracy: 0.6646\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.643704  Validation Accuracy: 0.6596\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.68779  Validation Accuracy: 0.6676\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.710099  Validation Accuracy: 0.666\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.624008  Validation Accuracy: 0.6664\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.593755  Validation Accuracy: 0.6658\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.649401  Validation Accuracy: 0.6622\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.685702  Validation Accuracy: 0.669\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.701575  Validation Accuracy: 0.6648\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.631944  Validation Accuracy: 0.6668\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.594427  Validation Accuracy: 0.6688\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.63354  Validation Accuracy: 0.6628\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.671246  Validation Accuracy: 0.6712\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.678442  Validation Accuracy: 0.6674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.623539  Validation Accuracy: 0.6692\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.592722  Validation Accuracy: 0.6626\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.635308  Validation Accuracy: 0.6658\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.656394  Validation Accuracy: 0.6686\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.687633  Validation Accuracy: 0.6652\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.598795  Validation Accuracy: 0.669\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.573012  Validation Accuracy: 0.6708\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.638489  Validation Accuracy: 0.66\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.647457  Validation Accuracy: 0.675\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.66132  Validation Accuracy: 0.671\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.599537  Validation Accuracy: 0.6636\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.5651  Validation Accuracy: 0.6696\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.613562  Validation Accuracy: 0.6688\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.646687  Validation Accuracy: 0.6748\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.669978  Validation Accuracy: 0.6708\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.582715  Validation Accuracy: 0.6664\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.557359  Validation Accuracy: 0.6736\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.602467  Validation Accuracy: 0.673\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.643001  Validation Accuracy: 0.6736\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.655519  Validation Accuracy: 0.6674\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.574444  Validation Accuracy: 0.6724\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.554832  Validation Accuracy: 0.6696\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.610084  Validation Accuracy: 0.668\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.624199  Validation Accuracy: 0.6712\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.65269  Validation Accuracy: 0.6734\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.583545  Validation Accuracy: 0.6712\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.559444  Validation Accuracy: 0.6716\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.59481  Validation Accuracy: 0.6706\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.617767  Validation Accuracy: 0.6768\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.640323  Validation Accuracy: 0.6734\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.572664  Validation Accuracy: 0.6752\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.543166  Validation Accuracy: 0.6714\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.591809  Validation Accuracy: 0.6686\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.614541  Validation Accuracy: 0.6802\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.631849  Validation Accuracy: 0.6708\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.566157  Validation Accuracy: 0.67\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.541968  Validation Accuracy: 0.676\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.588611  Validation Accuracy: 0.6766\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.601559  Validation Accuracy: 0.6758\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.630589  Validation Accuracy: 0.6718\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.549484  Validation Accuracy: 0.6768\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.530035  Validation Accuracy: 0.6774\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.569767  Validation Accuracy: 0.6706\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.617815  Validation Accuracy: 0.6778\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.62372  Validation Accuracy: 0.674\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.56025  Validation Accuracy: 0.6732\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.525682  Validation Accuracy: 0.6812\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.567946  Validation Accuracy: 0.6688\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.590505  Validation Accuracy: 0.6768\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.614288  Validation Accuracy: 0.6754\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.553146  Validation Accuracy: 0.675\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.524231  Validation Accuracy: 0.68\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.571841  Validation Accuracy: 0.6746\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.591423  Validation Accuracy: 0.6774\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.604607  Validation Accuracy: 0.6752\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.536252  Validation Accuracy: 0.674\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.518724  Validation Accuracy: 0.6784\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.556551  Validation Accuracy: 0.6746\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.587974  Validation Accuracy: 0.6792\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.613085  Validation Accuracy: 0.6772\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.517049  Validation Accuracy: 0.6756\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.536604  Validation Accuracy: 0.6732\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.548972  Validation Accuracy: 0.676\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.564497  Validation Accuracy: 0.6834\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.60595  Validation Accuracy: 0.6762\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.539844  Validation Accuracy: 0.6738\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.508305  Validation Accuracy: 0.6792\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.567027  Validation Accuracy: 0.6718\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.569019  Validation Accuracy: 0.6804\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.593972  Validation Accuracy: 0.6748\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.52228  Validation Accuracy: 0.6736\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.500097  Validation Accuracy: 0.6852\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.545592  Validation Accuracy: 0.67\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.562246  Validation Accuracy: 0.6754\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.5792  Validation Accuracy: 0.6804\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.521628  Validation Accuracy: 0.675\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.507278  Validation Accuracy: 0.6784\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.538618  Validation Accuracy: 0.676\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.565892  Validation Accuracy: 0.6826\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.581477  Validation Accuracy: 0.6758\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.511812  Validation Accuracy: 0.68\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.496727  Validation Accuracy: 0.6762\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.535103  Validation Accuracy: 0.6742\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.570525  Validation Accuracy: 0.6812\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.573582  Validation Accuracy: 0.6784\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.505834  Validation Accuracy: 0.6774\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.487833  Validation Accuracy: 0.6798\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.520155  Validation Accuracy: 0.6696\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.564179  Validation Accuracy: 0.6818\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.558515  Validation Accuracy: 0.6762\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.512948  Validation Accuracy: 0.6716\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.486917  Validation Accuracy: 0.6792\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.528626  Validation Accuracy: 0.6784\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.541513  Validation Accuracy: 0.6808\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.553779  Validation Accuracy: 0.6818\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.486199  Validation Accuracy: 0.6816\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.479119  Validation Accuracy: 0.68\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.513338  Validation Accuracy: 0.6752\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.5354  Validation Accuracy: 0.6792\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.550609  Validation Accuracy: 0.6798\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.497575  Validation Accuracy: 0.6798\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.470801  Validation Accuracy: 0.6792\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.505673  Validation Accuracy: 0.675\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.535458  Validation Accuracy: 0.681\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.540846  Validation Accuracy: 0.68\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.49232  Validation Accuracy: 0.6788\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.46179  Validation Accuracy: 0.6776\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.503927  Validation Accuracy: 0.6808\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.533868  Validation Accuracy: 0.6858\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.53228  Validation Accuracy: 0.6846\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.478445  Validation Accuracy: 0.683\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.476359  Validation Accuracy: 0.682\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.50414  Validation Accuracy: 0.6766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.523091  Validation Accuracy: 0.6852\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.538784  Validation Accuracy: 0.6832\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.472758  Validation Accuracy: 0.6782\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.463795  Validation Accuracy: 0.6836\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.49742  Validation Accuracy: 0.676\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.529839  Validation Accuracy: 0.6816\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.532996  Validation Accuracy: 0.6782\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.460073  Validation Accuracy: 0.6836\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.464062  Validation Accuracy: 0.6808\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.494893  Validation Accuracy: 0.6838\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.517182  Validation Accuracy: 0.6866\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.517472  Validation Accuracy: 0.688\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.468765  Validation Accuracy: 0.6826\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.462032  Validation Accuracy: 0.6854\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.498884  Validation Accuracy: 0.6768\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.50904  Validation Accuracy: 0.6798\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.53045  Validation Accuracy: 0.6836\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.447128  Validation Accuracy: 0.6828\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.45276  Validation Accuracy: 0.687\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.486449  Validation Accuracy: 0.6792\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.500656  Validation Accuracy: 0.6912\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.510505  Validation Accuracy: 0.6848\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.443468  Validation Accuracy: 0.6868\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.449043  Validation Accuracy: 0.688\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.493108  Validation Accuracy: 0.6818\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.503586  Validation Accuracy: 0.6876\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.50674  Validation Accuracy: 0.6842\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.454085  Validation Accuracy: 0.6864\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.451037  Validation Accuracy: 0.687\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.486405  Validation Accuracy: 0.6794\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.493576  Validation Accuracy: 0.6872\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.502719  Validation Accuracy: 0.6826\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.440574  Validation Accuracy: 0.683\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.423499  Validation Accuracy: 0.6856\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.464409  Validation Accuracy: 0.6808\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.495726  Validation Accuracy: 0.6858\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.517883  Validation Accuracy: 0.6836\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.449102  Validation Accuracy: 0.6792\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.426452  Validation Accuracy: 0.6918\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.462452  Validation Accuracy: 0.6834\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.474493  Validation Accuracy: 0.6872\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.511938  Validation Accuracy: 0.6864\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.441597  Validation Accuracy: 0.6802\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.423473  Validation Accuracy: 0.6862\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.469952  Validation Accuracy: 0.6812\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.471067  Validation Accuracy: 0.6858\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.486064  Validation Accuracy: 0.6816\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.446354  Validation Accuracy: 0.6786\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.433737  Validation Accuracy: 0.6854\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.465627  Validation Accuracy: 0.6852\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.474988  Validation Accuracy: 0.6848\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.479043  Validation Accuracy: 0.682\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.439205  Validation Accuracy: 0.686\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.426143  Validation Accuracy: 0.6866\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.47817  Validation Accuracy: 0.6876\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.464393  Validation Accuracy: 0.6832\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.469991  Validation Accuracy: 0.6852\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.421607  Validation Accuracy: 0.6792\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.43488  Validation Accuracy: 0.683\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.476753  Validation Accuracy: 0.6838\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.465089  Validation Accuracy: 0.6876\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.482868  Validation Accuracy: 0.6892\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.420616  Validation Accuracy: 0.679\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.412436  Validation Accuracy: 0.6852\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.461324  Validation Accuracy: 0.6866\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.46268  Validation Accuracy: 0.6808\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.471697  Validation Accuracy: 0.6878\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.42444  Validation Accuracy: 0.6806\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.423454  Validation Accuracy: 0.6846\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.466067  Validation Accuracy: 0.6788\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.457848  Validation Accuracy: 0.6844\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.477428  Validation Accuracy: 0.6812\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.419628  Validation Accuracy: 0.682\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.412665  Validation Accuracy: 0.6884\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.44792  Validation Accuracy: 0.6806\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.460605  Validation Accuracy: 0.6894\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.45523  Validation Accuracy: 0.6828\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.426841  Validation Accuracy: 0.6766\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.416665  Validation Accuracy: 0.6796\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.448246  Validation Accuracy: 0.6822\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.449028  Validation Accuracy: 0.6876\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.465735  Validation Accuracy: 0.6818\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.406119  Validation Accuracy: 0.676\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.40459  Validation Accuracy: 0.6864\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.443074  Validation Accuracy: 0.6768\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.454196  Validation Accuracy: 0.6856\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.454176  Validation Accuracy: 0.6786\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.425009  Validation Accuracy: 0.6794\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.406541  Validation Accuracy: 0.6804\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.429124  Validation Accuracy: 0.6834\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.435666  Validation Accuracy: 0.6876\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.451835  Validation Accuracy: 0.6914\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.40064  Validation Accuracy: 0.6854\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.399762  Validation Accuracy: 0.681\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.423233  Validation Accuracy: 0.6886\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.452511  Validation Accuracy: 0.6884\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.44585  Validation Accuracy: 0.684\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.408166  Validation Accuracy: 0.681\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.404713  Validation Accuracy: 0.6872\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.438242  Validation Accuracy: 0.6834\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.442  Validation Accuracy: 0.6864\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.458478  Validation Accuracy: 0.6862\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.388753  Validation Accuracy: 0.6858\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.415716  Validation Accuracy: 0.6836\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.408909  Validation Accuracy: 0.6832\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.430346  Validation Accuracy: 0.6834\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.446852  Validation Accuracy: 0.6828\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.386176  Validation Accuracy: 0.6866\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.399864  Validation Accuracy: 0.6854\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.425927  Validation Accuracy: 0.6804\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.433785  Validation Accuracy: 0.6854\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.447232  Validation Accuracy: 0.6836\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.385606  Validation Accuracy: 0.689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.393055  Validation Accuracy: 0.6838\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.423016  Validation Accuracy: 0.6858\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.418082  Validation Accuracy: 0.6864\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.432954  Validation Accuracy: 0.6812\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.393229  Validation Accuracy: 0.6822\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.394436  Validation Accuracy: 0.6828\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.419909  Validation Accuracy: 0.6798\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.434996  Validation Accuracy: 0.6872\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.434183  Validation Accuracy: 0.6834\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.390937  Validation Accuracy: 0.6812\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.382259  Validation Accuracy: 0.6858\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.423663  Validation Accuracy: 0.6848\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.421144  Validation Accuracy: 0.6838\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.436142  Validation Accuracy: 0.6856\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.385511  Validation Accuracy: 0.6814\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.383285  Validation Accuracy: 0.6844\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.42574  Validation Accuracy: 0.6844\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.414634  Validation Accuracy: 0.6844\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.429912  Validation Accuracy: 0.683\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.372568  Validation Accuracy: 0.6822\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.373226  Validation Accuracy: 0.6864\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.414036  Validation Accuracy: 0.682\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.434192  Validation Accuracy: 0.6822\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.418919  Validation Accuracy: 0.6798\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.388972  Validation Accuracy: 0.684\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.389298  Validation Accuracy: 0.684\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.401828  Validation Accuracy: 0.6836\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.416287  Validation Accuracy: 0.683\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.430152  Validation Accuracy: 0.6838\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.380765  Validation Accuracy: 0.6846\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.375775  Validation Accuracy: 0.6846\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.400398  Validation Accuracy: 0.6832\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.406282  Validation Accuracy: 0.6848\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.398686  Validation Accuracy: 0.685\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.387631  Validation Accuracy: 0.6812\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.381805  Validation Accuracy: 0.6888\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.404928  Validation Accuracy: 0.6798\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.409672  Validation Accuracy: 0.6894\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.407888  Validation Accuracy: 0.687\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.373901  Validation Accuracy: 0.6836\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.378629  Validation Accuracy: 0.6808\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.399608  Validation Accuracy: 0.6832\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.40114  Validation Accuracy: 0.6844\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.412717  Validation Accuracy: 0.6844\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.370327  Validation Accuracy: 0.6872\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.37443  Validation Accuracy: 0.6806\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.393927  Validation Accuracy: 0.6816\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6810489416122436\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x582e865240>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
